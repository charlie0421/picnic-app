import 'dart:convert';
import 'dart:io';
import 'dart:math';

import 'package:flutter/foundation.dart';
import 'package:flutter_test/flutter_test.dart';
import 'package:picnic_lib/core/utils/startup_performance_analyzer.dart';
import 'package:picnic_lib/core/utils/startup_profiler.dart';

/// ì•± ì‹œì‘ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸
///
/// ì´ í…ŒìŠ¤íŠ¸ëŠ” ì•± ì‹œì‘ ì„±ëŠ¥ì„ ì—¬ëŸ¬ ë²ˆ ì¸¡ì •í•˜ì—¬
/// í‰ê·  ì„±ëŠ¥ê³¼ ì„±ëŠ¥ ë³€ë™ì„±ì„ í™•ì¸í•©ë‹ˆë‹¤.
void main() {
  group('ì•± ì‹œì‘ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬', () {
    late StartupProfiler profiler;

    setUp(() {
      profiler = StartupProfiler();
      profiler.reset();
    });

    tearDown(() {
      profiler.reset();
    });

    test('ì‹œì‘ ì„±ëŠ¥ ê¸°ì¤€ì„  ì¸¡ì •', () async {
      // Given
      const testIterations = 5;
      final measurements = <Map<String, dynamic>>[];

      // When - ì—¬ëŸ¬ ë²ˆ ì¸¡ì •í•˜ì—¬ í‰ê· ê°’ ê³„ì‚°
      for (int i = 0; i < testIterations; i++) {
        profiler.reset();

        // ì‹œë®¬ë ˆì´ì…˜ëœ ì•± ì‹œì‘ ê³¼ì •
        await _simulateAppStartup(profiler);

        final results = profiler.getResults();
        if (results.isNotEmpty) {
          measurements.add(results);
        }

        // ì¸¡ì • ê°„ ê°„ê²©
        await Future.delayed(const Duration(milliseconds: 100));
      }

      // Then
      expect(measurements.length, equals(testIterations));

      // í‰ê·  ì„±ëŠ¥ ê³„ì‚°
      final averageMetrics = _calculateAverageMetrics(measurements);

      // ì„±ëŠ¥ ë³€ë™ì„± í™•ì¸ (í‘œì¤€í¸ì°¨ê°€ í‰ê· ì˜ 30% ì´í•˜ì—¬ì•¼ í•¨)
      final variabilityCheck = _checkPerformanceVariability(measurements);

      // ê²°ê³¼ ì¶œë ¥
      debugPrint('ğŸ“Š ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼:');
      debugPrint('í‰ê·  ì´ ì‹œì‘ ì‹œê°„: ${averageMetrics['total_startup_time_ms']}ms');
      debugPrint('ì„±ëŠ¥ ë³€ë™ì„±: ${variabilityCheck['coefficient_of_variation']}%');

      // ì„±ëŠ¥ ë³€ë™ì„±ì´ 30% ì´í•˜ì¸ì§€ í™•ì¸
      expect(variabilityCheck['coefficient_of_variation'], lessThan(30.0));

      // ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥
      await _saveBenchmarkResults(averageMetrics, variabilityCheck);
    });

    test('ì„±ëŠ¥ ë¶„ì„ ì‹œìŠ¤í…œ ê²€ì¦', () async {
      // Given
      await _simulateAppStartup(profiler);

      // When
      final analysis =
          await StartupPerformanceAnalyzer.analyzeCurrentPerformance();

      // Then
      expect(analysis, isNotEmpty);
      expect(analysis['performance_score'], isA<int>());
      expect(analysis['analysis'], isNotEmpty);
      expect(analysis['recommendations'], isA<List>());

      // ì„±ëŠ¥ ì ìˆ˜ê°€ 0-100 ë²”ìœ„ì¸ì§€ í™•ì¸
      final score = analysis['performance_score'] as int;
      expect(score, greaterThanOrEqualTo(0));
      expect(score, lessThanOrEqualTo(100));

      debugPrint('ì„±ëŠ¥ ì ìˆ˜: $score/100');
    });

    test('ë³‘ëª© ì§€ì  ì‹ë³„ í…ŒìŠ¤íŠ¸', () async {
      // Given - ì˜ë„ì ìœ¼ë¡œ ëŠë¦° ë‹¨ê³„ ì‹œë®¬ë ˆì´ì…˜
      profiler.startProfiling();

      profiler.startPhase('fast_phase');
      await Future.delayed(const Duration(milliseconds: 50));
      profiler.endPhase('fast_phase');

      profiler.startPhase('slow_phase');
      await Future.delayed(const Duration(milliseconds: 400)); // ì„ê³„ê°’ ì´ˆê³¼
      profiler.endPhase('slow_phase');

      profiler.markFirstFrame();
      profiler.finishProfiling();

      // When
      final analysis =
          await StartupPerformanceAnalyzer.analyzeCurrentPerformance();

      // Then
      final bottlenecks = analysis['bottlenecks'] as List<dynamic>;
      expect(bottlenecks, isNotEmpty);

      // slow_phaseê°€ ë³‘ëª©ìœ¼ë¡œ ì‹ë³„ë˜ì—ˆëŠ”ì§€ í™•ì¸
      final slowPhaseBottleneck = bottlenecks.firstWhere(
        (b) => b['phase'] == 'slow_phase',
        orElse: () => null,
      );
      expect(slowPhaseBottleneck, isNotNull);
      expect(slowPhaseBottleneck['severity'], equals('critical'));
    });

    test('ê¸°ì¤€ì„  ë¹„êµ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸', () async {
      // Given - ê¸°ì¤€ì„  ìƒì„±
      await _simulateAppStartup(profiler);
      await StartupPerformanceAnalyzer.saveAsBaseline();

      // When - ìƒˆë¡œìš´ ì¸¡ì • (ì•½ê°„ ë‹¤ë¥¸ ì„±ëŠ¥)
      profiler.reset();
      profiler.startProfiling();

      profiler.startPhase('test_phase');
      await Future.delayed(const Duration(milliseconds: 120)); // ê¸°ì¤€ì„ ë³´ë‹¤ ëŠë¦¼
      profiler.endPhase('test_phase');

      profiler.markFirstFrame();
      profiler.finishProfiling();

      final analysis =
          await StartupPerformanceAnalyzer.analyzeCurrentPerformance();

      // Then
      expect(analysis['comparison'], isNotNull);
      final comparison = analysis['comparison'] as Map<String, dynamic>;
      expect(comparison['improved'], isA<List>());
      expect(comparison['degraded'], isA<List>());
      expect(comparison['unchanged'], isA<List>());

      // ì •ë¦¬
      await _cleanupTestFiles();
    });
  });
}

/// ì•± ì‹œì‘ ê³¼ì •ì„ ì‹œë®¬ë ˆì´ì…˜í•©ë‹ˆë‹¤
Future<void> _simulateAppStartup(StartupProfiler profiler) async {
  profiler.startProfiling();

  // Flutter ë°”ì¸ë”© ì´ˆê¸°í™” ì‹œë®¬ë ˆì´ì…˜
  profiler.startPhase('flutter_bindings');
  await Future.delayed(const Duration(milliseconds: 80));
  profiler.endPhase('flutter_bindings');

  // ê¸°ë³¸ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹œë®¬ë ˆì´ì…˜
  profiler.startPhase('basic_services');
  await Future.delayed(const Duration(milliseconds: 120));
  profiler.endPhase('basic_services');

  // Firebase ì´ˆê¸°í™” ì‹œë®¬ë ˆì´ì…˜
  profiler.startPhase('firebase_init');
  await Future.delayed(const Duration(milliseconds: 200));
  profiler.endPhase('firebase_init');

  // Supabase ì´ˆê¸°í™” ì‹œë®¬ë ˆì´ì…˜
  profiler.startPhase('supabase_init');
  await Future.delayed(const Duration(milliseconds: 150));
  profiler.endPhase('supabase_init');

  // ì¸ì¦ ì„œë¹„ìŠ¤ ì‹œë®¬ë ˆì´ì…˜
  profiler.startPhase('auth_service');
  await Future.delayed(const Duration(milliseconds: 100));
  profiler.endPhase('auth_service');

  // ì•± ìœ„ì ¯ ìƒì„± ì‹œë®¬ë ˆì´ì…˜
  profiler.startPhase('app_widget_creation');
  await Future.delayed(const Duration(milliseconds: 50));
  profiler.endPhase('app_widget_creation');

  profiler.markFirstFrame();
  profiler.finishProfiling();
}

/// ì—¬ëŸ¬ ì¸¡ì •ê°’ì˜ í‰ê· ì„ ê³„ì‚°í•©ë‹ˆë‹¤
Map<String, dynamic> _calculateAverageMetrics(
    List<Map<String, dynamic>> measurements) {
  if (measurements.isEmpty) return {};

  final averages = <String, dynamic>{};

  // ì´ ì‹œì‘ ì‹œê°„ í‰ê· 
  final totalTimes = measurements
      .map((m) => m['total_startup_time_ms'] as int?)
      .where((t) => t != null)
      .cast<int>()
      .toList();

  if (totalTimes.isNotEmpty) {
    averages['total_startup_time_ms'] =
        (totalTimes.reduce((a, b) => a + b) / totalTimes.length).round();
  }

  // ë‹¨ê³„ë³„ í‰ê·  ê³„ì‚°
  final allPhases = <String>{};
  for (final measurement in measurements) {
    final phases = measurement['phase_durations'] as Map<String, dynamic>?;
    if (phases != null) {
      allPhases.addAll(phases.keys);
    }
  }

  final phaseAverages = <String, dynamic>{};
  for (final phase in allPhases) {
    final phaseTimes = measurements
        .map((m) =>
            (m['phase_durations'] as Map<String, dynamic>?)?[phase] as int?)
        .where((t) => t != null)
        .cast<int>()
        .toList();

    if (phaseTimes.isNotEmpty) {
      phaseAverages[phase] =
          (phaseTimes.reduce((a, b) => a + b) / phaseTimes.length).round();
    }
  }

  averages['phase_durations'] = phaseAverages;

  return averages;
}

/// ì„±ëŠ¥ ë³€ë™ì„±ì„ í™•ì¸í•©ë‹ˆë‹¤
Map<String, dynamic> _checkPerformanceVariability(
    List<Map<String, dynamic>> measurements) {
  if (measurements.length < 2) return {'coefficient_of_variation': 0.0};

  final totalTimes = measurements
      .map((m) => m['total_startup_time_ms'] as int?)
      .where((t) => t != null)
      .cast<int>()
      .toList();

  if (totalTimes.isEmpty) return {'coefficient_of_variation': 0.0};

  // í‰ê·  ê³„ì‚°
  final mean = totalTimes.reduce((a, b) => a + b) / totalTimes.length;

  // í‘œì¤€í¸ì°¨ ê³„ì‚°
  final variance =
      totalTimes.map((t) => (t - mean) * (t - mean)).reduce((a, b) => a + b) /
          totalTimes.length;
  final standardDeviation = sqrt(variance);

  // ë³€ë™ê³„ìˆ˜ ê³„ì‚° (í‘œì¤€í¸ì°¨ / í‰ê·  * 100)
  final coefficientOfVariation = (standardDeviation / mean) * 100;

  return {
    'mean': mean,
    'standard_deviation': standardDeviation,
    'coefficient_of_variation': coefficientOfVariation,
    'min': totalTimes.reduce((a, b) => a < b ? a : b),
    'max': totalTimes.reduce((a, b) => a > b ? a : b),
  };
}

/// ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤
Future<void> _saveBenchmarkResults(
  Map<String, dynamic> averageMetrics,
  Map<String, dynamic> variabilityCheck,
) async {
  final results = {
    'timestamp': DateTime.now().toIso8601String(),
    'average_metrics': averageMetrics,
    'variability_analysis': variabilityCheck,
    'test_environment': {
      'debug_mode': kDebugMode,
      'platform': Platform.operatingSystem,
    },
  };

  try {
    final file = File('startup_benchmark_results.json');
    await file.writeAsString(jsonEncode(results));
    debugPrint('ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ê°€ startup_benchmark_results.jsonì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤');
  } catch (e) {
    debugPrint('ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: $e');
  }
}

/// í…ŒìŠ¤íŠ¸ íŒŒì¼ë“¤ì„ ì •ë¦¬í•©ë‹ˆë‹¤
Future<void> _cleanupTestFiles() async {
  final filesToCleanup = [
    'startup_baseline.json',
    'startup_performance_report.json',
    'startup_benchmark_results.json',
  ];

  for (final filename in filesToCleanup) {
    try {
      final file = File(filename);
      if (await file.exists()) {
        await file.delete();
      }
    } catch (e) {
      debugPrint('íŒŒì¼ ì •ë¦¬ ì‹¤íŒ¨ ($filename): $e');
    }
  }
}
